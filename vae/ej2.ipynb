{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T17:49:54.513608Z",
     "start_time": "2024-06-25T17:49:52.296846Z"
    }
   },
   "source": [
    "from emoji import *\n",
    "from activation_functions import *\n",
    "from layer import *\n",
    "from variational_ae import *\n",
    "from mlp import *\n",
    "from utils import *\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config params"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T17:49:54.523334Z",
     "start_time": "2024-06-25T17:49:54.516484Z"
    }
   },
   "source": [
    "INPUT_ROWS = 20\n",
    "INPUT_COLS = 20\n",
    "INPUT_SIZE = INPUT_COLS * INPUT_ROWS\n",
    "LATENT_SIZE = 2\n",
    "HIDDEN_SIZE = 100\n",
    "HIDDEN_SIZE2 = 200\n",
    "HIDDEN_SIZE3 = 300\n",
    "EMOJIS_CHOSEN = len(emoji_images)\n",
    "NOISE = None\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-25T17:49:54.526157Z"
    }
   },
   "source": [
    "emoji_indexes = np.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "data = np.array(emoji_images)\n",
    "dataset_input = data[emoji_indexes]\n",
    "dataset_input_list = list(dataset_input)\n",
    "\n",
    "optimizer = Adam(0.001)\n",
    "\n",
    "encoder = MLP()\n",
    "encoder.addLayer(Dense(inputDim=INPUT_SIZE, outputDim=HIDDEN_SIZE3, activation=ReLU(), optimizer=optimizer))\n",
    "encoder.addLayer(Dense(inputDim=HIDDEN_SIZE3, outputDim=HIDDEN_SIZE2, activation=ReLU(), optimizer=optimizer))\n",
    "encoder.addLayer(Dense(inputDim=HIDDEN_SIZE2, outputDim=HIDDEN_SIZE, activation=ReLU(), optimizer=optimizer))\n",
    "\n",
    "sampler = Sampler(HIDDEN_SIZE, LATENT_SIZE, optimizer=optimizer)\n",
    "\n",
    "decoder = MLP()\n",
    "decoder.addLayer(Dense(inputDim=LATENT_SIZE, outputDim=HIDDEN_SIZE, activation=ReLU(), optimizer=optimizer))\n",
    "decoder.addLayer(Dense(inputDim=HIDDEN_SIZE, outputDim=HIDDEN_SIZE2, activation=ReLU(), optimizer=optimizer))\n",
    "decoder.addLayer(Dense(inputDim=HIDDEN_SIZE2, outputDim=HIDDEN_SIZE3, activation=ReLU(), optimizer=optimizer))\n",
    "decoder.addLayer(Dense(inputDim=HIDDEN_SIZE3, outputDim=INPUT_SIZE, activation=Sigmoid(), optimizer=optimizer))\n",
    "\n",
    "vae = VAE(encoder, sampler, decoder)\n",
    "\n",
    "vae.train(dataset_input=dataset_input_list, loss=MSE(), epochs=1000, callbacks={})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original vs Decoded"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "for i in range(len(dataset_input_list)):\n",
    "    input_reshaped = np.reshape(dataset_input_list[i], (len(dataset_input_list[i]), 1))\n",
    "    output = vae.feedforward(input_reshaped)\n",
    "    plot_data(list(dataset_input)[i], output, INPUT_ROWS, INPUT_COLS)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Samples Generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "for _ in range(15):\n",
    "    n = 10\n",
    "    digit_size = INPUT_ROWS\n",
    "    images = np.zeros((INPUT_ROWS, INPUT_COLS * n))\n",
    "\n",
    "    random_index1 = np.random.choice(emoji_indexes)\n",
    "    input_reshaped1 = np.reshape(emoji_images[random_index1], (len(emoji_images[random_index1]), 1))\n",
    "    vae.feedforward(input_reshaped1)\n",
    "    img1 = vae.sampler.sample\n",
    "\n",
    "    random_index2 = np.random.choice(emoji_indexes)\n",
    "    while random_index1 == random_index2:\n",
    "        random_index2 = np.random.choice(emoji_indexes)\n",
    "    input_reshaped2 = np.reshape(emoji_images[random_index2], (len(emoji_images[random_index2]), 1))\n",
    "    vae.feedforward(input_reshaped2)\n",
    "    img2 = vae.sampler.sample\n",
    "    \n",
    "    for i in range(n):\n",
    "        z = (img1 * (n - 1 - i) + img2 * i) / (n - 1)\n",
    "        output = vae.decoder.feedforward(z)\n",
    "        output = output.reshape(INPUT_ROWS, INPUT_COLS)\n",
    "        images[:, i * INPUT_COLS:(i + 1) * INPUT_COLS] = output\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(f\"\\\"{emoji_names[random_index1]}\\\" \"\n",
    "                  f\"-> \\\"{emoji_names[random_index2]}\\\"\")\n",
    "    plt.imshow(images, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving through the latent space"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "for j in range(10):\n",
    "    n = 10\n",
    "    digit_size = INPUT_ROWS\n",
    "    images = np.zeros((INPUT_ROWS, INPUT_COLS * n))\n",
    "\n",
    "    random_index1 = np.random.choice(emoji_indexes)\n",
    "    input_reshaped1 = np.reshape(emoji_images[random_index1], (len(emoji_images[random_index1]), 1))\n",
    "    vae.feedforward(input_reshaped1)\n",
    "    img1 = vae.sampler.sample\n",
    "\n",
    "    random_index2 = np.random.choice(emoji_indexes)\n",
    "    while random_index1 == random_index2:\n",
    "        random_index2 = np.random.choice(emoji_indexes)\n",
    "    input_reshaped2 = np.reshape(emoji_images[random_index2], (len(emoji_images[random_index2]), 1))\n",
    "    vae.feedforward(input_reshaped2)\n",
    "    img2 = vae.sampler.sample\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        z = np.array([i*0.2 - 0.5, j *0.2 - 0.5])\n",
    "        #print(z)\n",
    "        output = vae.decoder.feedforward(z)\n",
    "        output = output.reshape(INPUT_ROWS, INPUT_COLS)\n",
    "        images[:, i * INPUT_COLS:(i + 1) * INPUT_COLS] = output\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(images, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
